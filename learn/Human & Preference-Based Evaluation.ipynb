{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Human & Preference-Based Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated metrics don’t always align with human judgments, so LLM evaluation requires human feedback.\n",
    "\n",
    "2.1 Human Rating Scales\n",
    "\n",
    "Likert Scale – 1 to 5 rating for fluency, coherence, etc.\n",
    "\n",
    "Ranking-based Evaluation – Compare multiple outputs and rank them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likert Scale – 1 to 5 Rating <br>\n",
    "Human evaluators rate text quality on a 1 to 5 scale based on:<br>\n",
    "\t•\tFluency: Is the text grammatically correct?<br>\n",
    "\t•\tCoherence: Does it make sense in context?<br>\n",
    "\t•\tRelevance: Does it answer the question or match the task?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] Generated Text: The cat sits on the mat.\n",
      "\n",
      "[2] Generated Text: Cat mat on sits.\n",
      "\n",
      "[3] Generated Text: A feline is resting on a carpet.\n",
      "\n",
      "**Ranked Outputs (Best to Worst):**\n",
      "1. Cat mat on sits. (Avg Score: 5.00)\n",
      "2. A feline is resting on a carpet. (Avg Score: 3.67)\n",
      "3. The cat sits on the mat. (Avg Score: 3.33)\n"
     ]
    }
   ],
   "source": [
    "def collect_human_ratings():\n",
    "    outputs = [\n",
    "        \"The cat sits on the mat.\",\n",
    "        \"Cat mat on sits.\",\n",
    "        \"A feline is resting on a carpet.\"\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    for idx, output in enumerate(outputs):\n",
    "        print(f\"\\n[{idx + 1}] Generated Text: {output}\")\n",
    "        fluency = int(input(\"Rate Fluency (1-5): \"))\n",
    "        coherence = int(input(\"Rate Coherence (1-5): \"))\n",
    "        relevance = int(input(\"Rate Relevance (1-5): \"))\n",
    "        \n",
    "        avg_score = (fluency + coherence + relevance) / 3\n",
    "        scores.append((output, avg_score))\n",
    "    \n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\n**Ranked Outputs (Best to Worst):**\")\n",
    "    for rank, (text, score) in enumerate(scores, start=1):\n",
    "        print(f\"{rank}. {text} (Avg Score: {score:.2f})\")\n",
    "\n",
    "# Run the human evaluation collection\n",
    "collect_human_ratings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking-Based Evaluation\n",
    "\n",
    "Instead of assigning numerical scores, humans compare multiple outputs and rank them in order of preference.\n",
    "\n",
    "Example: Comparing Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose the better response:\n",
      "[1] Climate change means the planet is getting hotter due to pollution, affecting weather and ecosystems.\n",
      "[2] Global warming increases CO2 levels, causing environmental changes.\n",
      "\n",
      "Choose the better response:\n",
      "[1] Climate change means the planet is getting hotter due to pollution, affecting weather and ecosystems.\n",
      "[2] Rising heat is bad.\n",
      "\n",
      "Choose the better response:\n",
      "[1] Global warming increases CO2 levels, causing environmental changes.\n",
      "[2] Rising heat is bad.\n",
      "\n",
      "**Final Ranking:**\n",
      "1. Climate change means the planet is getting hotter due to pollution, affecting weather and ecosystems. (Wins: 2)\n",
      "2. Global warming increases CO2 levels, causing environmental changes. (Wins: 1)\n",
      "3. Rising heat is bad. (Wins: 0)\n"
     ]
    }
   ],
   "source": [
    "def compare_outputs():\n",
    "    outputs = [\n",
    "        \"Climate change means the planet is getting hotter due to pollution, affecting weather and ecosystems.\",\n",
    "        \"Global warming increases CO2 levels, causing environmental changes.\",\n",
    "        \"Rising heat is bad.\"\n",
    "    ]\n",
    "    \n",
    "    rankings = []\n",
    "    for i in range(len(outputs)):\n",
    "        for j in range(i + 1, len(outputs)):\n",
    "            print(f\"\\nChoose the better response:\\n[1] {outputs[i]}\\n[2] {outputs[j]}\")\n",
    "            choice = int(input(\"Enter 1 or 2: \"))\n",
    "            rankings.append((outputs[i], outputs[j], choice))\n",
    "    \n",
    "    # Count wins and rank outputs\n",
    "    scores = {output: 0 for output in outputs}\n",
    "    for output1, output2, choice in rankings:\n",
    "        if choice == 1:\n",
    "            scores[output1] += 1\n",
    "        else:\n",
    "            scores[output2] += 1\n",
    "    \n",
    "    # Sort outputs by ranking\n",
    "    sorted_outputs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n**Final Ranking:**\")\n",
    "    for rank, (text, score) in enumerate(sorted_outputs, start=1):\n",
    "        print(f\"{rank}. {text} (Wins: {score})\")\n",
    "\n",
    "# Run pairwise ranking\n",
    "compare_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "Collect human preference data (A > B style ranking).\n",
    "\n",
    "Train a reward model on this data.\n",
    "\n",
    "Fine-tune LLMs using RL with PPO (Proximal Policy Optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qunta-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
