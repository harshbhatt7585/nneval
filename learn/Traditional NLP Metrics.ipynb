{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional NLP Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "BLEU compares the n-gram overlap between the generated and reference text. It measures how much the generated text resembles the reference text.\n",
    "\n",
    "\t‚Ä¢\tPrecision-based: Measures how many n-grams in the candidate exist in the reference\n",
    "\n",
    "\t‚Ä¢\tBrevity Penalty: Penalizes short outputs to prevent cheating.\n",
    "    \n",
    "\t‚Ä¢\tRange: 0 (bad) to 1 (perfect match)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ BLEU = BP \\times \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Example 1\n",
    "reference = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]]\n",
    "candidate = [\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "bleu_score = sentence_bleu(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")  # Perfect match, should be 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.1221\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "reference = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]]\n",
    "candidate = [\"the\", \"cat\", \"sits\", \"on\", \"the\", \"floor\"]\n",
    "\n",
    "bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words ‚Äúsits‚Äù and ‚Äúfloor‚Äù do not match the reference, lowering the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU (1-gram): 0.6667\n",
      "BLEU (2-gram): 0.5164\n",
      "BLEU (3-gram): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Example 3 - Testing different n-gram weights\n",
    "bleu_1gram = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))  # Unigrams\n",
    "bleu_2gram = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))  # Bigrams\n",
    "bleu_3gram = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))  # Trigrams\n",
    "\n",
    "print(f\"BLEU (1-gram): {bleu_1gram:.4f}\")\n",
    "print(f\"BLEU (2-gram): {bleu_2gram:.4f}\")\n",
    "print(f\"BLEU (3-gram): {bleu_3gram:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "How ROUGE Works\n",
    "\n",
    "ROUGE measures recall-based word overlap, used mainly for summarization.\n",
    "\n",
    "Types of ROUGE\n",
    "\n",
    "\t1.\tROUGE-1: Unigram (single word) overlap.\n",
    "\n",
    "\t2.\tROUGE-2: Bigram (two consecutive words) overlap.\n",
    "\t\n",
    "\t3.\tROUGE-L: Longest Common Subsequence (LCS).\n",
    "\n",
    "ROUGE Formula\n",
    "\n",
    "\n",
    "$$ ROUGE = \\frac{|Overlapping\\ words|}{|Words\\ in\\ reference|} $$\n",
    "\n",
    "It calculates Precision, Recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge-score)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk->rouge-score) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk->rouge-score) (4.66.4)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=3046cb3a98d41d4e3d04b8394fddd2db5bf2e737f6ef475958d09baa19cab898\n",
      "  Stored in directory: /Users/harshbhatt/Library/Caches/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.1.0 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Precision=0.6667, Recall=0.6667, F1-Score=0.6667\n",
      "rouge2: Precision=0.4000, Recall=0.4000, F1-Score=0.4000\n",
      "rougeL: Precision=0.6667, Recall=0.6667, F1-Score=0.6667\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Example 1\n",
    "reference = \"the cat is on the mat\"\n",
    "candidate = \"the cat sits on the floor\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference, candidate)\n",
    "\n",
    "# Display scores\n",
    "for key, value in scores.items():\n",
    "    print(f\"{key}: Precision={value.precision:.4f}, Recall={value.recall:.4f}, F1-Score={value.fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n",
      "rouge2: Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n",
      "rougeL: Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - Perfect match\n",
    "reference = \"the cat is on the mat\"\n",
    "candidate = \"the cat is on the mat\"\n",
    "\n",
    "scores = scorer.score(reference, candidate)\n",
    "for key, value in scores.items():\n",
    "    print(f\"{key}: Precision={value.precision:.4f}, Recall={value.recall:.4f}, F1-Score={value.fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Precision=0.1250, Recall=0.1111, F1-Score=0.1176\n",
      "rouge2: Precision=0.0000, Recall=0.0000, F1-Score=0.0000\n",
      "rougeL: Precision=0.1250, Recall=0.1111, F1-Score=0.1176\n"
     ]
    }
   ],
   "source": [
    "# Example 3 - More different sentence\n",
    "reference = \"The quick brown fox jumps over the lazy dog\"\n",
    "candidate = \"A fast fox leaps above a sleepy canine\"\n",
    "\n",
    "scores = scorer.score(reference, candidate)\n",
    "for key, value in scores.items():\n",
    "    print(f\"{key}: Precision={value.precision:.4f}, Recall={value.recall:.4f}, F1-Score={value.fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR (Metric for Evaluation of Translation with Explicit ORdering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METEOR improves BLEU by considering:\n",
    "Synonym Matching (e.g., ‚Äúfast‚Äù and ‚Äúquick‚Äù are considered the same)\n",
    "\n",
    "Stem Matching (e.g., ‚Äúrunning‚Äù and ‚Äúrun‚Äù are matched)\n",
    "\n",
    "Recall & Precision Balance\n",
    "\n",
    "Word Order Consideration\n",
    "\n",
    "üîπ Range: 0 (bad) to 1 (perfect)\n",
    "\n",
    "üîπ Best for: Translation & Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/harshbhatt/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.6250\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Example 1 - Basic METEOR Calculation\n",
    "reference = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]]  # List of lists (each reference tokenized)\n",
    "candidate = [\"the\", \"cat\", \"sits\", \"on\", \"the\", \"floor\"]  # Tokenized candidate sentence\n",
    "\n",
    "score = meteor_score(reference, candidate)\n",
    "print(f\"METEOR Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.2871\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - Handling Synonyms & Stemming\n",
    "reference = [[\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]]\n",
    "candidate = [\"a\", \"fast\", \"fox\", \"leaps\", \"above\", \"a\", \"sleepy\", \"canine\"]\n",
    "\n",
    "score = meteor_score(reference, candidate)\n",
    "print(f\"METEOR Score: {score:.4f}\")  # Should be higher than BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qunta-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
