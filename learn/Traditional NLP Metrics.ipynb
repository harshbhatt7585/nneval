{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional NLP Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "BLEU compares the n-gram overlap between the generated and reference text. It measures how much the generated text resembles the reference text.\n",
    "\n",
    "\t•\tPrecision-based: Measures how many n-grams in the candidate exist in the reference\n",
    "\n",
    "\t•\tBrevity Penalty: Penalizes short outputs to prevent cheating.\n",
    "    \n",
    "\t•\tRange: 0 (bad) to 1 (perfect match)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ BLEU = BP \\times \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Example 1\n",
    "reference = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]]\n",
    "candidate = [\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "bleu_score = sentence_bleu(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")  # Perfect match, should be 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.1221\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "reference = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"]]\n",
    "candidate = [\"the\", \"cat\", \"sits\", \"on\", \"the\", \"floor\"]\n",
    "\n",
    "bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words “sits” and “floor” do not match the reference, lowering the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU (1-gram): 0.6667\n",
      "BLEU (2-gram): 0.5164\n",
      "BLEU (3-gram): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/harshbhatt/miniconda3/envs/qunta-backend/lib/python3.11/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Example 3 - Testing different n-gram weights\n",
    "bleu_1gram = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))  # Unigrams\n",
    "bleu_2gram = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0))  # Bigrams\n",
    "bleu_3gram = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0))  # Trigrams\n",
    "\n",
    "print(f\"BLEU (1-gram): {bleu_1gram:.4f}\")\n",
    "print(f\"BLEU (2-gram): {bleu_2gram:.4f}\")\n",
    "print(f\"BLEU (3-gram): {bleu_3gram:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qunta-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
